{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1bba77",
   "metadata": {},
   "source": [
    "# Step 1: Install Torch\n",
    "Execute the following cell which will install torch and torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8526d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\adi\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adi\\anaconda3\\lib\\site-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\adi\\anaconda3\\lib\\site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\adi\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\adi\\anaconda3\\lib\\site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\adi\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\adi\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adi\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\adi\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adi\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.9.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91ee74",
   "metadata": {},
   "source": [
    "# Step 2: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch import optim \n",
    "from torchvision import datasets \n",
    "from torchvision import transforms \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8c9708",
   "metadata": {},
   "source": [
    "# Step 3: Download the CIFAR10 dataset\n",
    "Excute the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b8f0160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path='Downloads/' \n",
    "cifar10=datasets.CIFAR10(data_path,train=True,download=True) \n",
    "cifar10_val=datasets.CIFAR10(data_path,train=False,download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac675c9",
   "metadata": {},
   "source": [
    "# Step 4: Explore the dataset\n",
    "See the type of cifar10\n",
    "Get the length of cifar10\n",
    "Assign image and label of cifar10 at index 1000\n",
    "Get the class name of label\n",
    "HINT: Use cifar10.classes[label] to get the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d5f7621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49d129b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10),len(cifar10_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ea0e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,label=cifar10[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59b40346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'truck'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10.classes[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0c364",
   "metadata": {},
   "source": [
    "# Step 5: Visualize the image\n",
    "Use matplotlib to visuazlize image\n",
    "HINT: just use plt.imshow(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70a86125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f7d671c970>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d82ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ee231b4",
   "metadata": {},
   "source": [
    "# Step 6: Transform images\n",
    "We need to convert the PIL image to a PyTorch tensor\n",
    "We can easily transform it by adding transform=transforms.ToTensor() when reading the dataset.\n",
    "This is given below (just execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e394318",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_cifar10=datasets.CIFAR10(data_path,train=True,download=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa44fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08ef459c",
   "metadata": {},
   "source": [
    "# Step 7: Normalize images\n",
    "Now you have all images (transformed) in tensor_cifar10.\n",
    "To concatenate a stack of images use torch.stack(..., dim=3) on the images\n",
    "HINT: Use list comprehension to get a list of images from tensor_cifar10 (to exclude labels)\n",
    "Calculate the mean(dim=1) by applying it on the stack\n",
    "Calculate the std(dim=1) by applying it on the stack\n",
    "We will use the results in next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1047e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=torch.stack([img_t for img_t,_ in tensor_cifar10],dim=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d31a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.view(3,-1).mean(dim=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs.view(3,-1).std(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb29613",
   "metadata": {},
   "source": [
    "# Step 8: Normalize the data\n",
    "We can add a normalize transform with adding a transforms.Compose([...]), where the list will contain the transforms.\n",
    "The transform we want are transforms.ToTensor() and transforms.Normalize(...)\n",
    "HINT: See lesson how it was done\n",
    "The transforms.Normalize(...) takes two tuples of the results from last step.\n",
    "Note: that in the lesson it was single numbers, here we hare tuples.\n",
    "Read the datasets to cifar10 with the new transform\n",
    "Read the validation dataset to cifar10_val with the new transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef1197b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10=datasets.CIFAR10(data_path,train=True,download=False,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                    (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc1dc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val=datasets.CIFAR10(data_path,train=False,download=False,\n",
    "                                transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                    (0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ea83d",
   "metadata": {},
   "source": [
    "# Step 9: Limit the dataset\n",
    "There are 10 classes in this dataset - to simplify, we will reduce it to two\n",
    "We will keep label 0 and 2 ('airplane' and 'bird')\n",
    "Use list comprehension to filter the datasets.\n",
    "To simplify use a label_map = {0: 0, 2: 1}, which is used to map label 0 to 0 and label 2 to 1.\n",
    "Then use list comprehension [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\n",
    "And similar for cifar10_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c26b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1} \n",
    "cifar2=[(img, label_map[label]) for img, label in cifar10 if label in [0, 2]] \n",
    "cifar2_val=[(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d75178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad018e3",
   "metadata": {},
   "source": [
    "# Step 10: Create the model\n",
    "We create a simple model here\n",
    "3072 input nodes -> Linear with 512 nodes (Tanh acitivation) -> Linear with 2 nodes (LogSoftmax activation)\n",
    "To do that use nn.Sequential(...) with the following arguments.\n",
    "nn.Linear(3072, 512)\n",
    "Bonus question: Why 3072 input nodes?\n",
    "nn.Tanh()\n",
    "nn.Linear(512, 2)\n",
    "nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09d765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(nn.Linear(3072,512),\n",
    "                   nn.Tanh(),\n",
    "                   nn.LogSoftmax(dim=1))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69449260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546061b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e5d54b0",
   "metadata": {},
   "source": [
    "# Step 11: Train the model\n",
    "Prepare training data\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "Set the learning_rate = 0.01 (to make it easy to adjust)\n",
    "Prepare optimizer and loss function.\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.NLLLoss()\n",
    "Run the training\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72f5da51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss: 5.585826\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate) \n",
    "loss_fn = nn.NLLLoss() \n",
    "n_epochs = 10 \n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    batch_size = imgs.shape[0]\n",
    "    outputs = model(imgs.view(batch_size, -1))\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1220e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07677e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46bd5aa",
   "metadata": {},
   "source": [
    "# Step 12: Test the model\n",
    "Run the following code (where we assume the test data is called cifar10_val and the model model.\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8aed977f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: %f 0.515625\n",
      "Accuracy: %f 0.6015625\n",
      "Accuracy: %f 0.5885416666666666\n",
      "Accuracy: %f 0.58984375\n",
      "Accuracy: %f 0.578125\n",
      "Accuracy: %f 0.5859375\n",
      "Accuracy: %f 0.5892857142857143\n",
      "Accuracy: %f 0.58203125\n",
      "Accuracy: %f 0.5677083333333334\n",
      "Accuracy: %f 0.5671875\n",
      "Accuracy: %f 0.5696022727272727\n",
      "Accuracy: %f 0.5690104166666666\n",
      "Accuracy: %f 0.5673076923076923\n",
      "Accuracy: %f 0.5658482142857143\n",
      "Accuracy: %f 0.56875\n",
      "Accuracy: %f 0.572265625\n",
      "Accuracy: %f 0.5762867647058824\n",
      "Accuracy: %f 0.5772569444444444\n",
      "Accuracy: %f 0.5773026315789473\n",
      "Accuracy: %f 0.58046875\n",
      "Accuracy: %f 0.5788690476190477\n",
      "Accuracy: %f 0.5774147727272727\n",
      "Accuracy: %f 0.5788043478260869\n",
      "Accuracy: %f 0.5852864583333334\n",
      "Accuracy: %f 0.58375\n",
      "Accuracy: %f 0.5817307692307693\n",
      "Accuracy: %f 0.5798611111111112\n",
      "Accuracy: %f 0.5786830357142857\n",
      "Accuracy: %f 0.578125\n",
      "Accuracy: %f 0.5760416666666667\n",
      "Accuracy: %f 0.5766129032258065\n",
      "Accuracy: %f 0.57763671875\n",
      "Accuracy: %f 0.5767045454545454\n",
      "Accuracy: %f 0.5753676470588235\n",
      "Accuracy: %f 0.5794642857142858\n",
      "Accuracy: %f 0.5794270833333334\n",
      "Accuracy: %f 0.5772804054054054\n",
      "Accuracy: %f 0.5752467105263158\n",
      "Accuracy: %f 0.5733173076923077\n",
      "Accuracy: %f 0.5703125\n",
      "Accuracy: %f 0.5682164634146342\n",
      "Accuracy: %f 0.5654761904761905\n",
      "Accuracy: %f 0.5650436046511628\n",
      "Accuracy: %f 0.5639204545454546\n",
      "Accuracy: %f 0.5652777777777778\n",
      "Accuracy: %f 0.563858695652174\n",
      "Accuracy: %f 0.5615026595744681\n",
      "Accuracy: %f 0.560546875\n",
      "Accuracy: %f 0.5628188775510204\n",
      "Accuracy: %f 0.5653125\n",
      "Accuracy: %f 0.5655637254901961\n",
      "Accuracy: %f 0.5652043269230769\n",
      "Accuracy: %f 0.5645636792452831\n",
      "Accuracy: %f 0.5639467592592593\n",
      "Accuracy: %f 0.5619318181818181\n",
      "Accuracy: %f 0.5613839285714286\n",
      "Accuracy: %f 0.5605811403508771\n",
      "Accuracy: %f 0.5619612068965517\n",
      "Accuracy: %f 0.560646186440678\n",
      "Accuracy: %f 0.5609375\n",
      "Accuracy: %f 0.5601946721311475\n",
      "Accuracy: %f 0.5602318548387096\n",
      "Accuracy: %f 0.5602678571428571\n",
      "Accuracy: %f 0.560791015625\n",
      "Accuracy: %f 0.5608173076923076\n",
      "Accuracy: %f 0.5617897727272727\n",
      "Accuracy: %f 0.5615671641791045\n",
      "Accuracy: %f 0.5615808823529411\n",
      "Accuracy: %f 0.5602355072463768\n",
      "Accuracy: %f 0.5607142857142857\n",
      "Accuracy: %f 0.5602992957746479\n",
      "Accuracy: %f 0.5603298611111112\n",
      "Accuracy: %f 0.561429794520548\n",
      "Accuracy: %f 0.5616554054054054\n",
      "Accuracy: %f 0.560625\n",
      "Accuracy: %f 0.5610608552631579\n",
      "Accuracy: %f 0.5625\n",
      "Accuracy: %f 0.5629006410256411\n",
      "Accuracy: %f 0.5623022151898734\n",
      "Accuracy: %f 0.5615234375\n",
      "Accuracy: %f 0.5619212962962963\n",
      "Accuracy: %f 0.5619283536585366\n",
      "Accuracy: %f 0.5625\n",
      "Accuracy: %f 0.5628720238095238\n",
      "Accuracy: %f 0.5628676470588235\n",
      "Accuracy: %f 0.5623183139534884\n",
      "Accuracy: %f 0.5621408045977011\n",
      "Accuracy: %f 0.5623224431818182\n",
      "Accuracy: %f 0.5619733146067416\n",
      "Accuracy: %f 0.5618055555555556\n",
      "Accuracy: %f 0.5611263736263736\n",
      "Accuracy: %f 0.5608016304347826\n",
      "Accuracy: %f 0.561491935483871\n",
      "Accuracy: %f 0.5613364361702128\n",
      "Accuracy: %f 0.5606907894736842\n",
      "Accuracy: %f 0.5595703125\n",
      "Accuracy: %f 0.5597615979381443\n",
      "Accuracy: %f 0.5588329081632653\n",
      "Accuracy: %f 0.5580808080808081\n",
      "Accuracy: %f 0.55859375\n",
      "Accuracy: %f 0.5592512376237624\n",
      "Accuracy: %f 0.5603553921568627\n",
      "Accuracy: %f 0.5600728155339806\n",
      "Accuracy: %f 0.5599459134615384\n",
      "Accuracy: %f 0.5595238095238095\n",
      "Accuracy: %f 0.5591096698113207\n",
      "Accuracy: %f 0.5591413551401869\n",
      "Accuracy: %f 0.55859375\n",
      "Accuracy: %f 0.5580561926605505\n",
      "Accuracy: %f 0.5580965909090909\n",
      "Accuracy: %f 0.5581362612612613\n",
      "Accuracy: %f 0.55859375\n",
      "Accuracy: %f 0.558766592920354\n",
      "Accuracy: %f 0.557702850877193\n",
      "Accuracy: %f 0.5577445652173914\n",
      "Accuracy: %f 0.5577855603448276\n",
      "Accuracy: %f 0.5582264957264957\n",
      "Accuracy: %f 0.5577330508474576\n",
      "Accuracy: %f 0.5577731092436975\n",
      "Accuracy: %f 0.5571614583333333\n",
      "Accuracy: %f 0.5578512396694215\n",
      "Accuracy: %f 0.5578893442622951\n",
      "Accuracy: %f 0.5583079268292683\n",
      "Accuracy: %f 0.5588457661290323\n",
      "Accuracy: %f 0.558625\n",
      "Accuracy: %f 0.5580357142857143\n",
      "Accuracy: %f 0.5583169291338582\n",
      "Accuracy: %f 0.5587158203125\n",
      "Accuracy: %f 0.5589874031007752\n",
      "Accuracy: %f 0.5588942307692307\n",
      "Accuracy: %f 0.5588024809160306\n",
      "Accuracy: %f 0.5589488636363636\n",
      "Accuracy: %f 0.5583881578947368\n",
      "Accuracy: %f 0.558418843283582\n",
      "Accuracy: %f 0.5590277777777778\n",
      "Accuracy: %f 0.5587086397058824\n",
      "Accuracy: %f 0.5593065693430657\n",
      "Accuracy: %f 0.560348731884058\n",
      "Accuracy: %f 0.5603642086330936\n",
      "Accuracy: %f 0.5604910714285715\n",
      "Accuracy: %f 0.5606161347517731\n",
      "Accuracy: %f 0.5610695422535211\n",
      "Accuracy: %f 0.5617351398601399\n",
      "Accuracy: %f 0.5607638888888888\n",
      "Accuracy: %f 0.5602370689655173\n",
      "Accuracy: %f 0.5605736301369864\n",
      "Accuracy: %f 0.560905612244898\n",
      "Accuracy: %f 0.5607052364864865\n",
      "Accuracy: %f 0.5610318791946308\n",
      "Accuracy: %f 0.5613541666666667\n",
      "Accuracy: %f 0.5613617549668874\n",
      "Accuracy: %f 0.5610608552631579\n",
      "Accuracy: %f 0.5612745098039216\n",
      "Accuracy: %f 0.5608766233766234\n",
      "Accuracy: %f 0.5612903225806452\n",
      "Accuracy: %f 0.5613982371794872\n",
      "Accuracy: %f 0.5613\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "correct = 0 \n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        print(\"Accuracy: %f\", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fc2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61508042",
   "metadata": {},
   "source": [
    "# Step 13 (Optional): Improve the model\n",
    "Try to improve the model\n",
    "Simple things you can play with\n",
    "Adjust the learning rate\n",
    "Run more epochs\n",
    "Number of hidden nodes\n",
    "Medium things to play with\n",
    "Change activation functions\n",
    "Add another layer\n",
    "Advanced things\n",
    "Let your imagination guide you\n",
    "For inspiration see state of the art results (wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54100a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76555a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1210d54",
   "metadata": {},
   "source": [
    "# Step 14 (Optional): Add more classes\n",
    "The dataset was limited to two classes (airplanes and birds)\n",
    "Try to add another class (or more) and see how it changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491ebc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a69963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83414e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
